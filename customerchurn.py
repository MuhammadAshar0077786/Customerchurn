# -*- coding: utf-8 -*-
"""CustomerChurn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HM_aRQfJLiXLP2C5KG5lN1Tu9RTRH9Pc
"""

import pandas as pd
from sklearn.metrics import confusion_matrix, plot_confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

# df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
df = pd.read_csv('https://raw.githubusercontent.com/treselle-systems/customer_churn_analysis/master/WA_Fn-UseC_-Telco-Customer-Churn.csv')
print(df.shape)
df.head()
#یہاں میں نے آن لائن ڈیٹا سیٹ کو لیااور سب سے پہلے میں نے اس کی معلومات کو اکٹھا کیا

df.isna().sum()

df = df.iloc[:,1:]
print(df.shape)
df.columns.str.replace(" ", "_")

df.info()

# TotalCharges should be numeric either float or int so check all the other columns that look suspecious
# test all suspecious columns and change their data type
# df[['MultipleLines']]
df.describe()

# I also have suspecion that some columns have null values hidden i.e. missing valus were named something else
# for example '?' or dash or double dash like '-' or '--'. Let's check them.
# (df.loc[df['TotalCharges']== ' '])
missing_ = [' ', '?', '-', '--',]
# df.isin([' ']).sum()
df.isin(missing_).sum()

# replace the empty cells with zero, because in "Total Charges" if a cell is empty that means customer was not charged, so
# it can be replaced with zero easily.
# Upon inspection I found that the 11 cells have ' ' so let's put a zero there.
df.loc[df['TotalCharges']==" ", 'TotalCharges'] = 0
# df.MonthlyCharges.isin(' ').sum()
df['TotalCharges'] = pd.to_numeric(df['TotalCharges']) # turn the type to numeric
# from object because this column will be calculated later
# df.replace(' ', '_', regex=True, inplace=True) # another way to replace from one string to another is to use regex

# now seperate X and y
X = df.iloc[:,:-1]
y = df.iloc[:,-1]

"""## OneHotEncoding"""

# Using dummy variables from pandas is convenient over One Hot Encoding because ... trust me it just is!
# Now take all categorical columns and convert them into numerical ones
X_encoded = pd.get_dummies(X, columns=['gender', 'Partner', 'Dependents',
       'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',
       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
       'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'])
X = X_encoded.iloc[:,1:]

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

le.fit(y)
print(le.classes_)
y = le.fit_transform(y)
print(type(y))
print(len(y))
y

"""Doing one last thing <B>(Standard Scaling)</B> will complete <b>preprocessing
## <b>Standard Scaling
"""

# I have discovered that scaling features does not impact these kind of problems
# but let's just do it for the sake of nothing
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

"""## Split data set into training and testing parts"""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)

from sklearn.linear_model import LogisticRegression
logisticrressor = LogisticRegression()
logisticrressor.fit(X_train, y_train)
y_pred = logisticrressor.predict(X_test)
f1_metric_macro = f1_score(y_test, y_pred, average = "macro")
#average="macro" it calculates the sperate precision and recall of
# each class and than take the average of precision and recall. after it calculate the f1 score

print("F1 Score macro:",f1_metric_macro)
print("F1 Score macro:",f1_metric_micro)

accuracy_score(y_test, y_pred)*100 # 79.2%

from sklearn.svm import SVC
svr = SVC()
svr.fit(X_train, y_train)
y_pred = svr.predict(X_test)
print(accuracy_score(y_test, y_pred)*100) # accuracy 73.5%
plot_confusion_matrix(svr, X_test, y_test)

from sklearn.svm import SVC #rbf kernel
ksvr = SVC(kernel='rbf', degree=4, gamma='auto')
ksvr.fit(X_train, y_train)
y_pred = ksvr.predict(X_test)
print(accuracy_score(y_test, y_pred)*100) # accuracy 76.77%
plot_confusion_matrix(ksvr, X_test, y_test)

from sklearn.naive_bayes import GaussianNB
NaiveBay_classifier = GaussianNB()
NaiveBay_classifier.fit(X_train, y_train)
y_pred = NaiveBay_classifier.predict(X_test)
print("Naive Bayes ", accuracy_score(y_test, y_pred)*100) # 71%
plot_confusion_matrix(NaiveBay_classifier, X_test, y_test)

from sklearn.neighbors import KNeighborsClassifier
KNN_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
KNN_classifier.fit(X_train, y_train)
y_pred = KNN_classifier.predict(X_test)
accuracy_score(y_test, y_pred)*100 # accuracy 76.25%
plot_confusion_matrix(KNN_classifier, X_test, y_test)

from sklearn.ensemble import RandomForestClassifier
RandomForest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
RandomForest.fit(X_train, y_train)
y_pred = RandomForest.predict(X_test)
accuracy_score(y_test, y_pred)*100 # 78% accuracy
plot_confusion_matrix(RandomForest, X_test, y_test)

# iteratively check how many clusters would be best for the problem, so I will do it
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X_test)
accuracy_score(y_test, y_kmeans) # I don't know what messed it up but accuracy is below 20%

"""Every model did a job well done and best among them all was <b> Logistic Regression </B> that is because data is mostly lineraly seperable, that is why linear kernel of <b>SVM</b> performed better than radial basis funcion, <b>RBF</b>, kernel (or poly kernel, I gave it poly degree four, however, it could be increased to arbitrary number). An other very good predictor have always been <b> Random Forest</b> that is because though it is random, it takes data from many, many forests and finally it finds something that is good enough, it is quite fast as well, compared to svm with poly kernel (with a high degree of polynomial). Similarly <b>KNN</b> has been good at predicting for classificatoin, like this, where classes are seperable linearly, however, it can also perform well where data is clustered this is beauty of KNN but this was simple classification problem and it did a good job. However <b> Naive Bay</B> did worse than others for this problem, that is because it guesses things based on probability rather than finding a pattern in things.

Lastly <b>K means Clustering</b> technique was performed. Though there is similarity between classification and clustering however, they are not quite the same thing, for example sometimes data can be mapped on graph through a circle and it may appear in a circular form, though it would still be single class (classification problem) but it would not be possible for a simple clustering technique to perform as good for this task. This is what we see happening in this example. K means clustering was totally robbed of its glory by the problem (dataset) because datapoints were not clustered. The dataset has more than 3 dimensions so it can't be plotted. However, I will try to use data exatraction technique (if you extend my research time and allow me a few more days to submit this assignment) and bring it to two or three features (if they could describe the variation in between predicted and true values.

Your sincerely
Ashar
"""